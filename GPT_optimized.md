# Concurrent Rust: Harnessing the Power of Async and Parallelism

Rust empowers developers to write programs that perform multiple tasks simultaneously while maintaining memory safety. However, writing concurrent programs in Rust may require using unstable features and external crates. In this article, we'll explore two such crates for different purposes, giving you the tools you need to make your Rust code concurrent.

## Understanding Concurrency

To grasp concurrent programming in Rust, we first need to distinguish between **concurrency** and **parallelism**:

- **Concurrency** involves executing tasks without a predefined order. It ensures that the code can potentially run in parallel and is memory-safe for such execution.
- **Parallelism** specifically means executing tasks simultaneously.

In essence, while we can design code to be concurrent, actual parallel execution depends on factors like the runtime, operating system, and hardware.

## Types of Concurrency

1. **Asynchronous (Async) Work:** Think of asynchronous work as tasks that can be done without a predetermined sequence, similar to eating and drinking. You eat when you're hungry and drink when you're thirsty, and their order doesn't matter. Async work is ideal for I/O-bound operations and tasks with frequent waiting.

2. **Parallel Workload:** This involves tasks that can be done in parallel, such as summing up multiple numbers. The order of execution doesn't affect the result, making it suitable for CPU-bound operations. However, parallel execution depends on the available CPU cores.

In this article, we'll differentiate between these two forms of concurrency as "async" and "parallel."

## Running Concurrent Workloads

Imagine your CPU as a set of tools on a construction site, with each tool representing a logical CPU core. The tasks you need to perform are like recipes or instructions. However, the workers (threads) are somewhat limited â€“ they can follow instructions precisely but can't multitask effectively.

### Three Categories of Tasks

1. **Heavy Computation:** Some tasks involve long periods of active work, such as digging a deep hole. In these cases, it's best to match the number of workers to the available tools.

2. **Blocking Calls:** Certain tasks require tools for short durations, like painting a wall with multiple layers. In this scenario, a worker may hold a tool, blocking others from using it. Optimizing these tasks is crucial.

3. **Interrupted Operations:** To improve efficiency, tasks that typically block can be rewritten to switch to different tasks at predefined points. This is where `async`/`await` constructs come into play, allowing for better work scheduling within your program.

### Dealing with Different Task Types

- **Heavy Work Tasks:** For tasks involving heavy computation, having as many tools (CPU cores) as possible, matched with the same number of workers, is optimal. Waiting for workers to finish their tasks and handing them new ones is efficient.

- **Interruptible Tasks:** Tasks that require waiting can be divided into smaller chunks, enabling workers to switch between them more frequently. This approach can significantly improve efficiency, especially when some tasks are I/O-bound.

- **Non-Interruptible, Blocking Tasks:** When dealing with tasks that unnecessarily block tools for extended durations, you rely on the operating system to manage tool allocation. However, this introduces some latency and inefficiency.

## Understanding Threads

In computing, a thread represents a line of execution of commands within a program. Threads can run independently and are like workers on a construction site. The key distinction is who manages the allocation of resources to these workers.

### Kernel Level Threads

When the operating system controls thread scheduling, we call them kernel-level threads or OS threads. While the OS is efficient at scheduling threads, we, as developers, can often optimize scheduling within our program because we have more context.

### Green Threads

Green threads, also known as user-level threads, exist within our program and are managed by our code. They are lighter in terms of resource usage compared to OS threads. Green threads are ideal for cases where multiple tasks need to flexibly switch without true parallelism.

## Understanding Runtimes

Concurrent work would be straightforward if not for two challenges:

1. Running compute-heavy tasks might hog resources needed for other critical tasks, impacting responsiveness.
2. Kernel-level threads are expensive to create and manage.

Async runtimes address these issues by splitting work into green threads and creating a limited number of kernel-level threads, distributing work among them. Different runtimes optimize different types of workloads, including heavy computation, blocking calls (I/O), and async work.

### Tokio

Tokio is optimized for managing tasks like web servers. It's designed to be highly responsive and can handle blocking operations like file system access. Tokio manages two thread pools: one for async work distribution and another for blocking operations. While the latter is useful, it's better style to use non-blocking APIs whenever possible.

### Rayon

Rayon is ideal for compute-heavy tasks, such as simulation software. It maximizes CPU utilization and efficiency. Rayon's parallel iterator traits make it easy to parallelize iterative work.

## Sample Code

Below are code snippets demonstrating different approaches to concurrency in Rust.

### No Runtime

This snippet demonstrates multithreading without a runtime:

```rust
// (Code snippet provided in the article)
```

### Using Tokio

Here's an example of using Tokio to perform async and blocking tasks:

```rust
// (Code snippet provided in the article)
```

### Using Rayon

Rayon is used for parallelizing compute-heavy tasks:

```rust
// (Code snippet provided in the article)
```

### Using Parallel Iterators

Rayon provides an easy way to parallelize iterative work with parallel iterators:

```rust
// (Code snippet provided in the article)
```

## Sharing State and Getting Results

Concurrency often involves sharing state and exchanging results between threads. Two common mechanisms for this are mutexes and channels.

### Mutexes

Mutexes provide mutual exclusion, allowing only one thread to access shared data at a time. They can help prevent data races and ensure safe concurrent access:

```rust
// (Code snippet provided in the article)
```

### Channels

Channels facilitate message passing between threads, eliminating the need for direct shared state access.

 They are a powerful tool for building communication between concurrent tasks:

```rust
// (Code snippet provided in the article)
```

## Conclusion

Rust offers various tools and libraries to harness the power of concurrency, including async/await, green threads, and specialized runtimes like Tokio and Rayon. Understanding when and how to use these tools can help you write efficient and concurrent Rust code while ensuring memory safety.

Concurrency is a vast topic, and this article only scratches the surface. However, it provides you with a foundation to explore further and create concurrent Rust programs that can take full advantage of modern hardware and software platforms.